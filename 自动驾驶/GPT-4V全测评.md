[TOC]



# 题目

On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving

在GPT-4V（视觉）的道路上：自动驾驶 视觉-语言模型的 早期探索

# 概述

The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems

对自动驾驶技术的追求取决于:  **感知**、**决策**、**控制系统** 的复杂集成。

Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users.

传统的一些尝试，不管是 ​​ **数据驱动** 还是 **规则驱动** 的方法都受到阻碍，

因为没有能力去把握:one:复杂驾驶环境的细微差别 以及:two:其他道路用户的意图。

This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving.

这是一个非常重要的瓶颈，特别是在开发安全可靠的自动驾驶所需的常识理解和细致场景理解方面。

The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving.

视觉语言模型（**V**isual **L**anguage **M**odels）的出现代表了实现全自动驾驶汽车的新前沿。

This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios.

本报告提供了详尽的评估，内容是针对目前最好的VLM，GPT-4V（ision），及其自动驾驶场景的应用

We explore the model’s abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver.

我们探索了模型在以下方面的能力：理解和推理驾驶场景的能力、做出决策的能力和最终以驾驶员的身份行动的能力。

Our comprehensive tests span from basic scene recognition to complex causal reasoning and realtime decision-making under varying conditions.

我们的综合测试涵盖了从**基本的场景识别**到**复杂的因果推理**和**不同条件下的实时决策**。

Our findings reveal that GPT-4V demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems.

我们的研究结果发现，与现有的自主系统相比，GPT-4V在**场景理解**和**因果推理**方面表现优异

It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts.

它展示了在真实驾驶环境中处理非分布场景、识别意图和做出明智决策的潜力。

However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development.

然而，挑战依然存在，特别是在方向识别、交通灯识别、视觉基础和空间推理任务方面。这些限制强调了进一步研究和开发的必要性。

Project is now available on GitHub for interested parties to access and utilize: https://github.com/PJLab-ADG/GPT4V-AD-Exploration arXiv:2311.05332v1 [cs.CV] 9 Nov 2023

略

# 内容

![image-20231123154125417](F:\论文\毕设学习\未命名.assets\image-20231123154125417.png)

Figure 1: An illustration showing the transition from the traditional autonomous driving pipeline to the integration of visual language models such as GPT-4V. This picture is generated by DALL·E 3.

图1：一个示意图，展示了从传统的自动驾驶流水线过渡到集成的视觉语言模型（如GPT-4V）的示意图。此图由DALL.E3生成。

## 1、介绍

### 1.1 动机与概述(Motivation and Overview)

#### 1.1.1 第一段

The quest for fully autonomous vehicles has long been constrained by a pipeline that relies on perception, decision-making, and planning control systems

对全自动驾驶汽车的追求很长时间以来受到依赖于**感知、做出决策以及规划控制系统**的限制

Traditional approaches, whether rooted in data-driven algorithms or rule-based methods, fall short in several key areas

传统的方法，无论是基于数据驱动还是基于规则的方法，在几个关键领域都存在不足。

Specifically, they exhibit weaknesses in accurately perceiving open-vocabulary objects and struggle with interpreting the behavioral intentions of surrounding traffic participants.

具体来说，他们的弱点在准确感知开放词汇对象和解释周围交通参与者的行为意图。

The reason is that traditional approaches only characterize abstract features of limited acquisition data or deal with problems according to predetermined rules, whereas they lack the “common sense” to deal with rare but significant corner cases and fail to summarize driving-related knowledge from the data for nuanced scenario understanding and effective causal reasoning.

原因在于，传统的方法只能**描述有限采集数据的抽象特征**或**处理那些根据预先确定规则的问题**，但是缺乏处理罕见但是重要的边缘案例的“常识”，从而无法从数据中总结与驾驶相关的知识，以进行细致的场景理解和有效的因果推理。



#### 1.1.2 第二段

The emergence of Large Language Models (LLMs), exemplified by GPT-3.5 [12], GLM [7, 24], Llama [18, 19], et al, has shown a glimmer of hope in addressing these issues.

以GPT-3.5，GLM，LIama等为代表的大型语言模型（**L**arge **L**anguage **M**odels）的出现，为解决这些问题带来了一线希望。

The LLMs are equipped with a rudimentary form of common sense reasoning, thereby showing promise in understanding complex driving scenarios.

这些大型语言模型（LLMs）配备了基本形式的常识推理，因此在理解复杂的驾驶场景方面有优势。

However, their application in autonomous driving has been restricted mainly to decision-making and planning phases [8, 20, 5, 11].

但是，它们在自动驾驶中的应用主要局限于决策和规划阶段。

This limitation is due to their inherent inability to process and understand visual data, which is critical for accurately perceiving the driving environment and driving the vehicle safely

这种限制是由于它们固有的无法处理和理解视觉数据，但是这个能力这对于准确感知驾驶环境和安全驾驶车辆至关重要。

#### 1.1.3 第三段

The recent development of GPT-4V [15, 16, 13, 22], a cutting-edge Vision-Language Model (VLM), has opened up new vistas for research and development.

GPT-4V是一种前沿的视觉语言模型（**V**ision-**L**anguage **M**odel），最近的发展为研究和开发开辟了新的前景。

Unlike its predecessors (GPT-4 [14]), GPT-4V possesses robust capabilities in image understanding, marking a significant step forward in closing the perception gap in autonomous driving technologies.

与前身(GPT-4)不同，GPT-4V具有强大的图像理解能力，标志着在缩小自动驾驶技术感知差距方面迈出了重要的一步。

This newfound strength raises the question: Can GPT-4V serve as a cornerstone for improving scene understanding and causal reasoning in autonomous driving?

这种新发现的优势提出了一个问题：GPT-4V能否成为提高自动驾驶场景理解和因果推理能力的基石？

#### 1.1.4 第四段

In this paper, we aim to answer this pivotal question by conducting an exhaustive evaluation of GPT-4V’s abilities.

在本文中，我们的目标是通过对GPT-4V的能力进行详尽的评估来回答这个关键问题。

Our research delves into the model’s performance in the intricate aspects of scene understanding and causal reasoning within the domain of autonomous driving.

我们的研究深入研究了模型在自动驾驶领域的场景理解和因果推理等复杂方面的性能。

Through exhaustive testing and in-depth analysis, we have elucidated both the capabilities and limitations of GPT-4V, which is anticipated to offer valuable support for researchers to venture into potential future applications within the autonomous driving industry

通过详尽的测试和深入的分析，我们已经阐明了GPT-4V的能力和局限性，预计这将为研究人员探索自动驾驶行业潜在的未来应用提供宝贵的支持。

#### 1.1.5 第五段

We have tested the capabilities of GPT-4V with increasing difficulty, from scenario understanding to reasoning, and finally testing its continuous judgment and decision-making ability as drivers in real-world driving scenarios. Our exploration of GPT-4V in the field of autonomous driving mainly focuses on the following aspects:

我们对GPT-4V的能力进行了难度越来越高的测试，从场景理解到推理，最后在真实驾驶场景中测试其作为驾驶员的持续判断和决策能力。我们对GPT-4V在自动驾驶领域的探索主要集中在以下几个方面：

1、**Scenario Understanding**: This test aims to assess GPT-4V’s fundamental recognition abilities. It involves recognizing weather and illumination conditions while driving, identifying traffic lights and signs in various countries, and assessing the positions and actions of other traffic participants in photos taken by different types of cameras. 

1、**场景理解**：本测试旨在评估GPT-4V的基本识别能力。它包括在驾驶时识别天气和照明条件，识别不同国家的交通信号灯和标志，并通过不同类型的相机拍摄的照片评估其他交通参与者的位置和行动。

Additionally, we explored simulation images and point cloud images of different perspectives for curiosity’s sake.

此外，出于好奇，我们探索了不同视角的模拟图像和点云图像。

2、 **Reasoning**: In this phase of the test, we delve deeper into assessing GPT-4V’s causal reasoning abilities within autonomous driving contexts. This evaluation encompasses several crucial aspects. Firstly, we scrutinize its performance in tackling complex corner cases, which often challenge data-driven perception systems. Secondly, we assess its competence in providing a surround view, which is a vital feature in autonomous driving applications.

2、**推理**：在测试的这一阶段，我们将更深入地评估GPT-4V在自动驾驶环境下的因果推理能力。这项评估包括几个关键方面。

首先，我们仔细研究了它在处理复杂的极端情况时的性能，这些极端情况经常挑战

其次，我们评估了其提供环绕视角的能力，这是自动驾驶应用的一个重要特征。

Given GPT-4V’s inability to directly process video data, we utilize concatenated time series images as input to gauge its temporal correlation capabilities.

鉴于GPT-4V无法直接处理视频数据，我们利用连接的时间序列图像作为输入来衡量其时间相关能力。

Additionally, we conduct tests to validate its capacity to associate real-world scenes with navigation images, further examining its holistic understanding of autonomous driving scenarios.

此外，我们还进行了测试，以验证其将真实场景与导航图像联系起来的能力，进一步检验其对自动驾驶场景的整体理解。

3、 **Act as a driver**: To harness the full potential of GPT-4V, we entrusted it with the role of a seasoned driver, tasking it with making decisions in real driving situations based on the environment. 

3、**作为驾驶员**：为了充分发挥GPT-4V的潜力，我们赋予它经验丰富的驾驶员的角色，让它在真实的驾驶情况下根据环境做出决策。

Our approach involved sampling driving video at a consistent frame rate and feeding it to GPT-4V frame by frame.

我们的方法包括以一致的帧率对驾驶视频进行采样，并逐帧将其馈送到GPT-4V。

 To aid its decision-making, we supplied essential vehicle speed and other relevant information and communicated the driving objective for each video. 

为了帮助其决策，我们提供了必要的车速等相关信息，并传达了每个视频的驾驶目标。

We challenged GPT-4V to produce the necessary actions and provide explanations for its choices, thereby pushing the boundaries of its capabilities in real-world driving scenarios.

我们要求GPT-4V产生必要的行动，并为其选择提供解释，从而推动其在现实驾驶场景中的能力界限。

In conclusion, we offer initial insights as a foundation for inspiring future research endeavors in the realm of autonomous driving with GPT-4V. 

总之，我们提供了初步的见解，作为启发未来在GPT-4V自动驾驶领域的研究工作的基础。

Building upon the information presented above, we methodically structure and showcase the qualitative results of our investigation using a unique and engaging compilation of image-text pairs. 

在上述信息的基础上，我们系统地构建并展示了我们调查的定性结果：通过独特且引入入胜的图像-文本对汇编。

While this methodology may be somewhat less stringent, it affords the opportunity for a comprehensive analysis.

虽然这种方法可能不那么严格，但它提供了进行全面分析的机会。

### 1.2 指导(Guidance)

#### 1.2.1 第一段

This article focuses on testing in the field of autonomous driving, employing a curated selection of images and videos representing diverse driving scenarios. 

本文主要关注自动驾驶领域的测试，采用了代表不同驾驶场景的精选图像和视频。

The test samples are sourced from various outlets, including open-source datasets such as nuScenes [3], Waymo Open dataset [17], Berkeley Deep Drive-X (eXplanation) Dataset (BDD-X) [9], D2 -city [4], Car Crash Dataset (CCD) [2], TSDD [1], CODA [10], ADD [21], as well as V2X datasets like DAIR-V2X [23] and CitySim [25].

测试样本来自各种渠道，包括开源数据集，如nuScenes[3]、Waymo开放数据集[17]、Berkeley Deep Drive-X (eXplanation)数据集(BDD-X)[9]、D2 -city[4]、车祸数据集(CCD)[2]、TSDD[1]、CODA[10]、ADD[21]，以及V2X数据集，如DAIR-V2X[23]和CitySim[25]。

Additionally, some samples are derived from the CARLA [6] simulation environment, and others are obtained from the internet. 

此外，一些样本来自CARLA模拟环境，其他样本来自互联网。

It’s worth noting that the image data used in testing may include images with timestamps up to April 2023, potentially overlapping with the GPT-4V model’s training data, while the text queries employed in this article are entirely generated anew.

值得注意的是，测试中使用的图像数据可能包括时间戳截止到2023年4月的图像，可能与GPT-4V模型的训练数据重叠，而本文中使用的文本查询完全是重新生成的

#### 1.2.2 第二段

All experiments detailed in this paper were conducted before November 5th, 2023, utilizing the web-hosted GPT-4V(ision) (version from September 25th). 

本文中详细的所有实验都是在2023年11月5日之前进行的，使用的是web托管的GPT-4V(视觉)（9月25日版本）。

We acknowledge that the most recent version of GPT-4V, which has received updates following the November 6th OpenAI DevDay, may produce different responses when presented with the same images compared to our test results.

我们承认，最新版本的GPT-4V在11月6日OpenAI DevDay之后进行了更新，当呈现相同的图像时，与我们测试结果相比，可能会产生不同的反应。

## 2、情景理解的基本能力(Basic capability of Scenario Understanding)

To achieve safe and effective autonomous driving, a fundamental prerequisite is a thorough understanding of the current scenario. 

为了实现安全有效的自动驾驶，一个基本的先决条件就是彻底了解当前的场景。

Complex traffic scenarios encompass a myriad of driving conditions, each hosting a diverse array of traffic participants.

复杂的交通场景包含无数的驾驶条件，每个驾驶条件都有不同的交通参与者。

 Accurate recognition and comprehension of these elements serve as basic capabilities for an autonomous vehicle to make informed and appropriate driving decisions. 

准确识别和理解这些元素是自动驾驶汽车做出明智和适当的驾驶决策的基本能力。

In this section, we present a series of tests aimed at evaluating GPT-4V’s ability to comprehend traffic scenarios. 

在本节中，我们将介绍一系列旨在评估GPT-4V理解交通场景能力的测试。

We focus on two primary aspects: the model’s understanding of the surrounding environment and its capacity to discern the behavior and status of various traffic participants. 

我们主要关注两个方面：**模型对周围环境的理解**，以及**识别各种交通参与者的行为和状态**的能力。

Through these assessments, we aim to shed light on GPT-4V’s competence in interpreting the dynamic traffic environment.

通过这些评估，我们的目标是阐明GPT-4V在解释动态交通环境方面的能力。

### 2.1 了解环境(Understanding of Environment)

